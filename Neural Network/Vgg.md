# Very Deep Convolutional Networks for Large-Scale Image Recognition

## Abstract

* 调查在大规模图像识别设定中，卷积神经网络深度对其准确率的影响
* 主要贡献：
  * 使用具有非常小（3×3）卷积滤波器的架构对深度增加的网络进行全面评估，这表明通过将深度推至16-19层可以实现对现有技术配置的显着改进。 
  * 我们还表明，我们的表示可以很好地推广到其他数据集，在这些数据集中，它们可以实现最先进的结果。
    我们公开了两种表现最佳的ConvNet模型，以便进一步研究在计算机视觉中使用深度视觉表示。



## Introduction

* 随着ConvNets成为计算机视觉领域的一种商品，人们已经做了许多尝试来改进Krizhevsky等人的原始架构，为了达到更好的准确性。例如，利用较小的接收窗口尺寸和较小的第一卷积层步幅，另一项改进涉及在整个图像和多个尺度上密集地训练和测试网络
* 在本文中，我们声明了对于卷积神经网络架构的另一方面改进--**深度**。为此，我们修复了架构的其他参数，并通过**添加更多卷积层来稳定地增加网络的深度**，这是可行的，因为在所有层中使用非常小的（3×3）卷积滤波器。
* 文章结构
  * 第二节：描述本文的神经网络配置
  * 第三节：介绍图像分类培训和评估的细节
  * 第四节：在ILSVRC分类任务上比较配置
  * 第五节：总结



## ConvNet Configurations

​		在本节中，我们首先描述我们的ConvNet配置的通用布局（第2.1节），然后使用特定的配置来评估（第2.2节）。然后讨论我们的设计选择，并与现有技术进行比较。 

### Architecture

* 在训练过程中，本文卷积网络的输入是固定的224 * 224的RGB图像，所做的唯一预处理是从每个像素中减去在训练集上计算的平均RGB值。图像会经过一堆3 * 3的卷积层（其中3 * 3是能满足左/右，上/下，中心概念的最小尺寸）。在其中一种配置中，我们还使用1×1卷积滤波器，可以看作是输入通道的线性变换（后面是非线性）。卷积步长固定为1个像素，**卷积层输入数据的空间填充使得在卷积之后保留空间分辨率，即对于3×3卷积层，填充是1个像素**。空间池化由5个最大池化层，其中一些是跟随卷积层的。最大池化层窗口设置为2 * 2，步长为2
* 一堆卷积层（在不同的架构中具有不同的深度）由三个全连接层跟随：前两个每个具有4096个通道，第三个执行1000路ILSVRC分类，因此包含1000个通道（每个类一个），最后一层是soft-max层，全连接层的配置在所有网络中都是相同的。
* 所有隐藏层都配备有整流（ReLU（Krizhevsky等，2012））非线性，我们注意到，我们的网络（除了一个）都没有包含本地响应规范化（LRN）：如图4所示，这种归一化不会改善ILSVRC数据集的性能，但会导致内存消耗和计算时间增加。在适用的情况下，LRN层的参数是（Krizhevsky等，2012）的参数。



### Configurations

* 本文评估的ConvNet配置在表1中列出，每列一个。在下文中，我们将通过名称（A-E）来引用网络。所有配置均遵循上文中提供的通用设计，并且仅在深度上有所不同：从网络A中的11个权重层（8个转换层和3个FC层）到网络E中的19个权重层（16个转换层和3个FC层）。卷积层的宽度（通道数）相当小，从第一层中的64开始，然后在每个最大池层之后增加2倍，直到达到512。**LRN：局部响应归一化**

![1562571007732](C:\Users\nihaopeng\Deep Learning\images\1562571007732.png)

* 在表2中，我们报告了每种配置的参数数量。尽管深度很大，我们网中的权重数量和接受域不会大于更大卷积网络中的权重数（144M权重（Sermanet等，2014））。

![1562571019568](C:\Users\nihaopeng\Deep Learning\images\1562571019568.png)

### Discussion

* 首先，我们采用三个非线性整流层而不是一个非线性整流层，这使决策功能更具辨识力。
* 其次，我们减少了参数的数量



## Classification Framework

### Training

* 通过使用具有动量的小批量梯度下降（基于反向传播）优化多项逻辑回归目标来执行训练
* batch大小设置为256，动量为0.9，通过权重衰减和前两个完全连接的层的丢失规范化（dropoutratio设置为0.5）使训练正规化
* 我们推测，尽管参数数量较多，而且我们的网络深度比较（Krizhevsky等，2012），网络需要更少的时期来收敛，原因是：
  * （a）由更大的深度和更小的卷积滤波器尺寸所暗示的隐式规则化 
  * （b）某些层的预初始化

