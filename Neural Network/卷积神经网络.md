# 卷积神经网络

## 两种思维

* 对于稀疏矩阵来说，卷积网络具有**稀疏性**，即卷积核的大小远远小于输入数据矩阵的大小。
* **参数共享**指的是在特征提取过程中，一个模型在多个参数之中使用相同的参数，在传统的神经网络中，每个权重只对其连接的输入输出起作用，当其连接的输入输出元素结束后就不会再用到。参数共享指的是在卷积神经网络中，核的每一个元素都被用在输入的每一个位置上，在计算过程中只需学习一个参数集合就能把这个参数应用到所有的图片元素中



## 卷积函数

```python
tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, name=None)
```

- **input**：指需要做卷积的输入图像，要求是一个**Tensor**，具有**[batch, in_height, in_width, in_channels]**这样的shape，分别代表一个batch的图片数量，图片高、宽，图像通道数，要求类型为float32或者float64
- **filter**：相当于CNN中的卷积核，要求是一个Tensor，具有**[filter_height, filter_width, in_channels, out_channels]**这样的shape
- **strides**:卷积时在图像每一维的步长，是一个一维的向量，第一维和第四维默认为1，而第三维和第四维分别是平行和竖直滑行的步长距离
- **padding**:String类型的量，只能是**SAME**、**VALID**其中之一，这个值决定了不同的卷积方式
- **use_cudnn_on_gpu**:bool类型，是否使用cudnn加速，默认为true



## 池化运算

```python
tf.nn.max_pool(value, ksize, strides, padding, name=None)
```

- **value**：需要池化的输入，一般池化层接在卷积层后面，所以输入通常是**featuremap**，依然是**[batch, height, width, channels]**这样的shape
- **ksize**：池化窗口的大小，取一个四维向量，一般是**[1, height, width, 1]**，因为不需要在batch和channel上做池化，所以这两个维度设为1
- **strides**：与卷积类似，窗口在每一个维度上滑动的步长，一般也是**[1, stride, stride, 1]**
- **padding**：与卷积类似，可以去**VALID**或者**SAME**，返回一个Tensor



​		池化最重要的作用就是帮助输入的数据表示近似不变性，对于平移不变性指的是对输入的数据进行少量平移时，经过池化后的输出结果并不会发生改变。局部平移不变性是一个很有用的性质，尤其是当关心某个特征是否出现而不关心它出现的具体位置时。