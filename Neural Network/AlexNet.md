# AlexNet

## Intrduction

* 包含五个卷积层，其中一些有池化层（max-pooling），三个全连接层
* 为了避免过拟合，我们设计使用了一个**dropout**，随机使一些神经元失效，以此来保证适度拟合
* ![1562397048568](C:\Users\nihaopeng\Deep Learning\images\1562397048568.png)
* 目标识别类别的复杂性意味着这个问题（需要更大的学习类别容量）不能被专门解决，即使是用ImageNet这种规模的数据集，所以本模型需要一些先验知识来弥补所缺失的数据
* 卷积神经网络（CNN）的模型容量可以被深度和广度来控制，并且它们还对图像的性质（即**统计的平稳性和像素依赖性的局部性**）做出了强有力且大多数正确的假设。因此，与具有类似大小的层的标准**前馈神经网络**相比，CNN具有更少的连接和参数，因此它们更容易训练，而它们的理论上最佳性能可能仅略微更差。
* 尽管CNN具有吸引人的特性，并且尽管它们的本地架构相对有效，但是它们在大规模应用于**高分辨率图像**方面仍然过于昂贵。（但GPU和2D卷积结合有所改善，同时ImageNet数据集也足够大了）
* 文章贡献
  * 编写了一个高度优化的2D卷积GPU实现和训练卷积神经网络中固有的所有其他操作，开源。
  * 准确率高等等
  * 当显卡等设备提升的时候，模型可以进一步优化





## DataSet

* ImageNet是超过1500万个标记的高分辨率图像的数据集，属于大约22,000个类别。这些图像是从网上收集的，并由人类贴标机使用Amazon的Mechanical Turk众包工具进行标记。从2010年开始，作为Pascal视觉对象挑战赛的一部分，举办了名为ImageNet大规模视觉识别挑战赛（ILSVRC）的年度比赛。 
  ILSVRC使用ImageNet的一个子集，在1000个类别中分别拥有大约1000个图像。总之，大约有120万个训练图像，50,000个验证图像和150,000个测试图像。
* ILSVRC-2010是唯一可以获得测试集标签的ILSVRC版本，因此这是我们执行大部分实验的版本。由于我们也在ILSVRC-2012竞赛中输入了我们的模型，因此我们在第6节中报告了此版本数据集的结果，其中测试集标签不可用。在ImageNet上，习惯上报告两个错误率：top-1和top-5，其中前5个错误率是测试图像的分数，正确的标签不属于模型认为最可能的五个标签之中
* ImageNet由可变分辨率图像组成，而我们的系统需要恒定的输入维度。因此，我们将图像下采样到256×256的固定分辨率。给定矩形图像，我们首先重新缩放图像，使得短边长度为256，然后从结果中裁剪出中心256×256贴片。
  图片。我们没有以任何其他方式预处理图像，除了从每个像素减去训练集上的平均活动。因此，我们在像素的（居中）原始RGB值上训练我们的网络



## Architecture

![1562400726102](C:\Users\nihaopeng\Deep Learning\images\1562400726102.png)

以下为其模型的一些重点

### ReLU非线性

![1562402616026](C:\Users\nihaopeng\Deep Learning\images\1562402616026.png)

* 标准激活函数包括：sigmoid和tanh函数
* 就具有梯度下降的训练时间而言，这些饱和非线性比非饱和非线性$f(x)= max(0,x)$慢得多。

### 多GPU训练

* 采用的并行化方案基本上将一半内核（或神经元）放在每个GPU上，还有一个额外的技巧：GPU仅在某些层中进行通信。
* 这意味着，例如，第3层的内核从第2层中的所有内核映射获取输入。但是，第4层中的内核仅从位于同一GPU上的第3层中的那些内核映射获取输入。选择连通模式是交叉验证的一个问题，但这使我们能够精确调整通信量，直到它是计算量的可接受部分。

### 局部响应归一化

​		通过![1562403657992](C:\Users\nihaopeng\Deep Learning\images\1562403657992.png)表示通过在位置（x，y）处应用内核i然后应用ReLU非线性而计算的神经元的活动，响应标准化活动![1562403673328](C:\Users\nihaopeng\Deep Learning\images\1562403673328.png)由表达式给出：

![1562403690152](C:\Users\nihaopeng\Deep Learning\images\1562403690152.png)

其中总和在相同空间位置的n个“相邻”内核映射上运行，N是层中内核的总数。

​		内核映射的顺序当然是任意的，并且在训练开始之前确定。这种响应标准化实现了一种横向抑制形式，其受到真实神经元中发现的类型的启发，从而在使用不同内核计算的神经元输出之间产生大活动的竞争。

​		![1562403869952](C:\Users\nihaopeng\Deep Learning\images\1562403869952.png)这四个是超参

### 重叠池化

​		在CNN中池化层总结了相同内核映射中相邻神经元组的输出。更确切地说，池化层可以被认为是由间隔s个像素的合并单元网格组成，每个合并单元总结了以合并单元的位置为中心的大小为z×z的邻域。如果我们设置s，这是我们在整个网络中使用的，s = 2和z = 3。

### 整体架构

* 五层卷积层，三层全连接层
* 最后一个全连接层的输出被馈送到1000路softmax，其产生1000个类别标签上的分布
* 我们的网络**最大化多项逻辑回归目标**，这相当于最大化预测分布下正确标签的对数概率的训练案例的平均值
* 第二、第四和第五卷积层的内核仅连接到位于同一GPU上的前一层中的那些内核映射，第三卷积层的内核连接到第二层中的所有内核映射，全连接层中的神经元连接到前一层中的所有神经元，响应归一化层尾随着第一和第二卷积层，第3.4节中描述的最大池化层跟随响应归一化层以及第五卷积层，ReLU非线性应用于每个卷积和完全连接层的输出。
* 各层解释：
  * 第一层卷积层输入的图像是224 * 224 * 3，用了96个卷积核11 * 11 * 3，步长为4像素（代表在核映射中，邻近神经元可接受域之间的距离）
  * 第二层卷积层以第一层卷积层的输出作为输入，用256个 5 * 5 * 48的卷积核
  * 第三、四和五卷积层彼此连接而没有任何中间汇集或池化层
  * 第三卷积层有384个 3 * 3 * 256个卷积核，连着第二卷积层输出的归一和池化后的数据
  * 第四卷积层有384个 3 * 3 * 192的卷积核
  * 第五卷积层有256个 3 * 3 * 192的卷积核
  * 每个全连接层有4096个神经元



## Reducing Overfitting

由于参数太多，不可避免的会考虑过拟合的情况，主要采用两种办法

### 数据扩充

* 第一种形式的数据增强包括生成图像平移和水平反射
* 第二种形式的数据增强包括改变训练图像中RGB通道的强度

### Dropout

​		引入的技术称为“丢失”，包括将每个隐藏神经元的输出设置为零，概率为0.5。以这种方式“退出”的神经元对前向传递没有贡献，并且不参与反向传播。因此，每次输入时，神经网络都会采样不同的架构，但所有这些架构都会共享权重。该技术减少了神经元的复杂共同适应，因为神经元不能依赖于特定其他神经元的存在。因此，它被迫学习更强大的特征，这些特征与其他神经元的许多不同随机子集一起使用是有用的。在测试时，使用所有神经元但是将它们的输出乘以0.5，这是采用由指数多个丢失网络产生的预测分布的几何平均值的合理近似值。