# Deep-Learning（花书）笔记

## 1. 线性代数

### 范数

* 范数是将向量映射到非负值的函数![1563000193574](../images/1563000193574.png)
* p=2时，$L^2$范数称为欧几里得范数，表示从原点出发到向量$x$确定的点的欧几里得距离
* ![1563000304733](../images/1563000304733.png)



### 特征分解

![1563000738598](../images/1563000738598.png)



### 奇异值分解

![1563000784766](../images/1563000784766.png)



### 迹运算

![1563000859757](../images/1563000859757.png)



### 主成分分析（PCA）

* 使用$L^2$范数，在空间中有m个点，我们希望对这些点进行有损压缩。有损压缩表示我们使用更少的内存，但损失一些精度去存储这些点，希望损失的精度尽可能少。
* 为了使编码问题简单一些，PCA限制D的列向量彼此**正交**





## 2. 信息论基础

![1563184337601](../images/1563184337601.png)

### Bernoulli分布

![1563184528193](../images/1563184528193.png)

### Multinoulli分布

![1563184566677](../images/1563184566677.png)

### 高斯分布

![1563184600290](../images/1563184600290.png)

### 指数分布和Laplace分布

![1563184627999](../images/1563184627999.png)

### Dirac分布和经验分布

![1563184662898](../images/1563184662898.png)



## 3. SVM

*  一个重要创新点在于：核技巧
* 不输出概率，只输出类别

![image-20190726145544950](/Users/nihaopeng/个人/Git/Deep-learning/images/image-20190726145544950.png)



